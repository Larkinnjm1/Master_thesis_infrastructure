{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Generate-a-dictionary-of-all-specific-folders-for-analysis-on-a-specific-patient\" data-toc-modified-id=\"Generate-a-dictionary-of-all-specific-folders-for-analysis-on-a-specific-patient-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Generate a dictionary of all specific folders for analysis on a specific patient</a></span><ul class=\"toc-item\"><li><span><a href=\"#Copying-image-files-from-patient-derived-folders-into-image-specific-folder-types-with-renaming-of-files-to-include-all-pertinent-information-is-held\" data-toc-modified-id=\"Copying-image-files-from-patient-derived-folders-into-image-specific-folder-types-with-renaming-of-files-to-include-all-pertinent-information-is-held-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Copying image files from patient derived folders into image specific folder types with renaming of files to include all pertinent information is held</a></span></li><li><span><a href=\"#Dicom-writing-patients-to-folder-with-specific-file-name-str-format\" data-toc-modified-id=\"Dicom-writing-patients-to-folder-with-specific-file-name-str-format-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dicom writing patients to folder with specific file name str format</a></span></li><li><span><a href=\"#Ground-writing-patients-to-folder-with-specific-file-name-format\" data-toc-modified-id=\"Ground-writing-patients-to-folder-with-specific-file-name-format-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Ground writing patients to folder with specific file name format</a></span></li></ul></li><li><span><a href=\"#Stripping-out-file-information-Patient-id-etc-from-file-name-using-regex\" data-toc-modified-id=\"Stripping-out-file-information-Patient-id-etc-from-file-name-using-regex-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Stripping out file information Patient id etc from file name using regex</a></span></li><li><span><a href=\"#Generate-Fold-text-files-denoting-validation-and-training-and-test-folders-for-analysis\" data-toc-modified-id=\"Generate-Fold-text-files-denoting-validation-and-training-and-test-folders-for-analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generate Fold text files denoting validation and training and test folders for analysis</a></span></li><li><span><a href=\"#Move-data-on-a-per-patient-basis-to-different-folder-set-aka-test-or-validation-folders\" data-toc-modified-id=\"Move-data-on-a-per-patient-basis-to-different-folder-set-aka-test-or-validation-folders-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Move data on a per patient basis to different folder set aka test or validation folders</a></span></li><li><span><a href=\"#Rename-Ground-files-to-match-image-source-file-names\" data-toc-modified-id=\"Rename-Ground-files-to-match-image-source-file-names-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Rename Ground files to match image source file names</a></span></li><li><span><a href=\"#Function-calls-for-generating-file_dictionaries,-file-tuple-sets\" data-toc-modified-id=\"Function-calls-for-generating-file_dictionaries,-file-tuple-sets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Function calls for generating file_dictionaries, file tuple sets</a></span></li><li><span><a href=\"#Sub-sampler-creating-folder-called-fold1-and-fold2-for-specific-analysis-in-set-folder\" data-toc-modified-id=\"Sub-sampler-creating-folder-called-fold1-and-fold2-for-specific-analysis-in-set-folder-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Sub sampler creating folder called fold1 and fold2 for specific analysis in set folder</a></span></li><li><span><a href=\"#Sub-sampler-by-patient-to-specific-destination-folders\" data-toc-modified-id=\"Sub-sampler-by-patient-to-specific-destination-folders-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Sub sampler by patient to specific destination folders</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grey-level-intensity-methods-for-grey-level-setting\" data-toc-modified-id=\"Grey-level-intensity-methods-for-grey-level-setting-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Grey level intensity methods for grey level setting</a></span></li></ul></li><li><span><a href=\"#Determine-sub-sampling-based-on-class-balance\" data-toc-modified-id=\"Determine-sub-sampling-based-on-class-balance-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Determine sub sampling based on class balance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shutil import copyfile\n",
    "import ipdb\n",
    "import filecmp\n",
    "import random\n",
    "\n",
    "import imageio\n",
    "from difflib import SequenceMatcher\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd F:\\Biomedical images\\Train\\NIFTI_MR_512x512\\NIFTI_MR_512x512\\NIFTI_MR_512x512_png_256grey_lvl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dictionary of all specific folders for analysis on a specific patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_modes=['t1dual','t2spir','inphase','outphase','dicom_anon','ground','source']\n",
    "src_dir=r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256 seg_rename'#os.getcwd()\n",
    "pair_keyword=[['t1dual_inphase','ground'],['t1dual_outphase','ground'],['t2spir','ground']]\n",
    "dst_grnd_path=''\n",
    "dst_img_path=''\n",
    "grnd_sub_repl_tple_dict={'_ground':'_inphase','.png':'_grey_lvl_256.png'}\n",
    "src_sub_repl_tple_dict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict=gen_img_fileset_dict(src_dir,imgs_modes,key_word_sp=1,len_file_sp=-1)\n",
    "file_dict_src_keys=[k for k,v in file_dict.items() if k.find('source')!=-1]\n",
    "file_dict_src_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in file_dict.items():\n",
    "    print(type(k))\n",
    "    if k.find('source')!=-1:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying image files from patient derived folders into image specific folder types with renaming of files to include all pertinent information is held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir=r'D:\\Biomedical images\\Train\\Preprocess_steps\\NIFTI_MR_N4_DICOM\\8'\n",
    "trgt_dir=r'D:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename'\n",
    "trgt_keyword='masks'\n",
    "img_sub_str=['t2spir','inphase','outphase']\n",
    "grey_lvl_sub_str=sorted(['64grey_lvl','256grey_lvl'])\n",
    "img_types_broad=['t1dual','t2spir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dicom writing patients to folder with specific file name str format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating directory directory bsaed on keyword in parameters above. \n",
    "trgt_dir_roots=[trgt_root for trgt_root,_,_ in os.walk(trgt_dir) if trgt_root.lower().find(trgt_keyword)!=-1]\n",
    "\n",
    "for root,subdirs,files in os.walk(src_dir):\n",
    "    \n",
    "    if len(files)>0 and root.lower().find('dicom_anon')!=-1:\n",
    "        \n",
    "        #Getting target directories to write files to. \n",
    "        img_match=[img_type for img_type in img_sub_str if root.lower().find(img_type)!=-1]\n",
    "        trgt_path_match=[trgt_root_dir for trgt_root_dir in trgt_dir_roots if trgt_root_dir.find(img_match[0])!=-1]\n",
    "        \n",
    "        \n",
    "        #Iterate through files for renaming clause\n",
    "        for file in files:\n",
    "            #ipdb.set_trace()\n",
    "            trgt_path_match_dict=dict(zip(grey_lvl_sub_str,trgt_path_match))\n",
    "            file_b_name=os.path.splitext(file)[0]\n",
    "            #Getting patient name inserted into file\n",
    "            int_in_f_name=re.findall('d|\\d+',file_b_name)\n",
    "            pat_idx=file_b_name.find(int_in_f_name[0])\n",
    "\n",
    "            dst_file_pat=insert_str(file_b_name,\n",
    "                                              int_in_f_name[0],\n",
    "                                              '_',pat_idx)\n",
    "\n",
    "            #splitting braod image types in file name\n",
    "            img_type_brd=[x for x in img_types_broad if file_b_name.lower().find(x)!=-1]\n",
    "            img_brd_idx=dst_file_pat.lower().find(img_type_brd[0])\n",
    "            dst_file_img_typ_brd=insert_str(dst_file_pat,img_type_brd[0], '_',img_brd_idx)\n",
    "            \n",
    "            #reading file in for renaming file for method.\n",
    "            src_file_path=os.path.join(root,file) \n",
    "            tmp_arr=imageio.imread(src_file_path)\n",
    "            \n",
    "            \n",
    "            #adding grey scale and image size\n",
    "            for k,v in trgt_path_match_dict.items():\n",
    "                final_f_name='pat_id_'+dst_file_img_typ_brd+'_'+k+'_256x256.png'\n",
    "                strp_dbl_undr_scr=final_f_name.replace('__','_')\n",
    "                trgt_path_match_dict[k]=os.path.join(v,strp_dbl_undr_scr).lower()\n",
    "                \n",
    "                #Bytescaling image for analysis\n",
    "                tmp_arr_256=bytescaling(tmp_arr)\n",
    "                \n",
    "                if k.find('256grey_lvl')!=-1:\n",
    "                    #print(trgt_path_match_dict[k]) \n",
    "                    imageio.imwrite(trgt_path_match_dict[k],tmp_arr_256)\n",
    "                \n",
    "                elif k.find('64grey_lvl')!=-1:\n",
    "\n",
    "                    tmp_arr_64=quantize_igs(tmp_arr_256,64,'igs')\n",
    "                    imageio.imwrite(trgt_path_match_dict[k],tmp_arr_64)\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground writing patients to folder with specific file name format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir=r\"F:\\Biomedical images\\Train\\MR_raw\\8\"\n",
    "trgt_dir=r\"F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename\"\n",
    "trgt_keyword='masks'\n",
    "img_sub_str=['t2spir','t1dual']\n",
    "grey_lvl_sub_str=['64grey_lvl','256grey_lvl']\n",
    "img_types_broad=['t1dual','t2spir']\n",
    "img_types_spec=['t1dual','t2spir','inphase','outphase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating directory directory bsaed on keyword in parameters above. \n",
    "trgt_dir_roots=[trgt_root for trgt_root,_,_ in os.walk(trgt_dir) if trgt_root.lower().find(trgt_keyword)!=-1]\n",
    "\n",
    "for root,subdirs,files in os.walk(src_dir):\n",
    "    \n",
    "    if len(files)>0 and root.lower().find('ground')!=-1:#\n",
    "        #print(root)\n",
    "        #ipdb.set_trace()\n",
    "        #Getting target directories to write files to. \n",
    "        img_match=[img_type for img_type in img_types_spec if root.lower().find(img_type)!=-1]\n",
    "        trgt_path_match=[trgt_root_dir for trgt_root_dir in trgt_dir_roots if trgt_root_dir.find(img_match[0])!=-1]\n",
    "        #sorting file into appropriate naming list\n",
    "        files=sorted(files)\n",
    "        file_info=root.split('\\\\')\n",
    "        pat_no=file_info[4]\n",
    "        print(pat_no)\n",
    "        img_type=file_info[5].lower()\n",
    "        \n",
    "        #Getting broad image types        \n",
    "        slc_no_st=1\n",
    "        #Iterate through files for renaming clause\n",
    "        for file in files:\n",
    "            #Creating dictoinary of values\n",
    "            \n",
    "            if img_type=='t1dual':\n",
    "                st_file_name=['pat_id_'+pat_no+'_'+img_type+'_inphase_slice_no_'+str(slc_no_st),\n",
    "                             'pat_id_'+pat_no+'_'+img_type+'_outphase_slice_no_'+str(slc_no_st)]\n",
    "                grey_lvl_sub_str=sorted(['t1o_64grey_lvl','t1o_256grey_lvl','t1i_64grey_lvl','t1i_256grey_lvl'])\n",
    "            elif img_type=='t2spir':\n",
    "                st_file_name=['pat_id_'+pat_no+'_'+img_type+'_slice_no_'+str(slc_no_st)]\n",
    "                grey_lvl_sub_str=sorted(['64grey_lvl','256grey_lvl'])\n",
    "            else:\n",
    "                print('error no matching values')\n",
    "                \n",
    "            \n",
    "            trgt_path_match_dict=dict(zip(grey_lvl_sub_str,trgt_path_match))\n",
    "            \n",
    "            #reading file in for renaming file for method.\n",
    "            src_file_path=os.path.join(root,file) \n",
    "            tmp_arr=imageio.imread(src_file_path)\n",
    "\n",
    "            #adding grey scale and image size\n",
    "            for k,v in trgt_path_match_dict.items():\n",
    "                if k.find('t1i_')!=-1:\n",
    "                    final_f_name=st_file_name[0]+'_'+k[len('tli_'):]+'_256x256.png'\n",
    "                elif k.find('t1o_')!=-1:\n",
    "                    final_f_name=st_file_name[0]+'_'+k[len('t1o_'):]+'_256x256.png'\n",
    "                else:\n",
    "                    final_f_name=st_file_name[0]+'_'+k+'_256x256.png'\n",
    "                strp_dbl_undr_scr=final_f_name.replace('__','_')\n",
    "                trgt_path_match_dict[k]=os.path.join(v,strp_dbl_undr_scr).lower()\n",
    "                imageio.imwrite(trgt_path_match_dict[k],tmp_arr)\n",
    "                #Bytescaling image for analysis\n",
    "            if int(pat_no)>9:\n",
    "                print(trgt_path_match_dict)\n",
    "                \n",
    "\n",
    "        #Increasing slice no by 1 \n",
    "            slc_no_st+=1\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_str(string,sub_str, str_to_insert, index):\n",
    "    \"\"\"the purpose of this method is to insert strings at specific indices\"\"\"\n",
    "    return string[:index+len(sub_str)] + str_to_insert + string[index+len(sub_str):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stripping out file information Patient id etc from file name using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_lst=[]\n",
    "for file in mask_file_list:\n",
    "    \n",
    "    Pat_id_act=int(file[file.lower().find(Pat_str)+len(Pat_str):file.lower().find(img_type_str)])\n",
    "    \n",
    "    tmp_img_type=[x for x in img_types_lst if file.lower().find(x)!=-1]\n",
    "    \n",
    "    #Getting slice no from file name\n",
    "    slic_no_lwr_bnd=file.lower().find('slice_no_')\n",
    "    slic_no_uppr_bnd=file.lower().rfind('_512x512')\n",
    "    slice_no=int(file[slic_no_lwr_bnd+len('slice_no_'):slic_no_uppr_bnd])\n",
    "    \n",
    "    tmp_img=imageio.imread(file)\n",
    "    \n",
    "    tmp_img_uniq_val=np.unique(tmp_img,return_counts=True)\n",
    "    tmp_dict={}\n",
    "    for int_val,pixel_cnts in list(zip(tmp_img_uniq_val[0],tmp_img_uniq_val[1])):\n",
    "        int_val_key=mask_intensities[int_val]\n",
    "        \n",
    "        tmp_dict[int_val_key]=pixel_cnts\n",
    "    \n",
    "    tmp_dict['slice_no']=slice_no\n",
    "    tmp_dict['image_type']=tmp_img_type[0]\n",
    "    tmp_dict['Patient_id']=Pat_id_act\n",
    "    \n",
    "    final_lst.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting ground file intensity values to between 0 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_dir=r'D:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename - Copy'\n",
    "dst_dir=r'D:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename - Copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_conv_dict={0:0,63:1,126:2,189:3,252:4}\n",
    "conv_grnd_img_sing_label(src_dir,dst_dir,int_conv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_grnd_img_sing_label(src_dir,dst_dir,int_conv_dict):\n",
    "    \"\"\"Generate single number iterative from i.e from intensity values 0,63,126,189,252 to 0,1,2,3,4,5\"\"\"\n",
    "    \n",
    "    for root,subdir,files in os.walk(src_dir):\n",
    "        #ipdb.set_trace()\n",
    "        if (root.find('ground')!=-1 or root.find('masks')!=-1) and len(files)>0:\n",
    "            #Generating temporary file for analysis\n",
    "            for file in files:\n",
    "                \n",
    "                #Importing image for analysis \n",
    "                tmp_path=os.path.join(root,file)\n",
    "                print(tmp_path)\n",
    "                tmp_img=imageio.imread(tmp_path)\n",
    "                \n",
    "                #Performing image conversion to single integer\n",
    "                try:\n",
    "                    tmp_img_conv=np.vectorize(int_conv_dict.get,otypes=[np.uint8])(tmp_img)\n",
    "                    print(np.unique(tmp_img_conv))\n",
    "                    \n",
    "               \n",
    "                    imageio.imwrite(tmp_path,tmp_img_conv)\n",
    "                except TypeError as e:\n",
    "                    print('Image already converted:',tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fold text files denoting validation and training and test folders for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis being be performed via fold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data on a per patient basis to different folder set aka test or validation folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "dst_path=r'F:\\Biomedical images\\validation\\1_pat_no_32'\n",
    "pat_no=[32]\n",
    "\n",
    "def non_subsample_patients(dst_path,src_path,pat_no,substr='inphase'):\n",
    "    \"\"\"the purpose of this method is to copy all patients that match number\n",
    "    list pat_no and place into train and validation folders.\"\"\"\n",
    "\n",
    "    pat_no_str=['pat_id_'+str(x)+'_' for x in pat_no]\n",
    "    for root,subdir,files in os.walk(src_path):\n",
    "        if len(files)>0 and root.lower().find(substr)!=-1:\n",
    "            root_split=root.split('\\\\')\n",
    "            #Creating new directory based o nfile names\n",
    "            root_split_rejoin='\\\\'.join(root_split[-3:])\n",
    "            dst_dir=os.path.join(dst_path,root_split_rejoin)\n",
    "            if not os.path.exists(dst_dir):\n",
    "                os.makedirs(dst_dir)\n",
    "            file_nm_lst=[]\n",
    "            for file in files:\n",
    "                \n",
    "                file_chk=[x for x in pat_no_str if file.find(x)!=-1]\n",
    "                #Copying file sthat match patient id. \n",
    "                if len(file_chk)==1:\n",
    "                    #ipdb.set_trace()\n",
    "                    print(file)\n",
    "                    src_img_path=os.path.join(root,file)\n",
    "                    dst_img_path=os.path.join(dst_dir,file)\n",
    "                    shutil.copy(src_img_path, dst_img_path)\n",
    "                    file_nm_lst.append(os.path.basename(file))\n",
    "    #Return tuple list of test file.\n",
    "    return file_nm_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Ground files to match image source file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd F:\\Biomedical images\\Train\\NIFTI_MR_512x512 seg_rename\\NIFTI_MR_512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_lvl_str=['128','256','64']\n",
    "phase_lvl_str={'inphase':'_inphase','outphase':'_outphase','t2spir':''}\n",
    "for root,subdir,files in os.walk(os.getcwd()):\n",
    "    \n",
    "    if root.find('ground')!=-1:\n",
    "        #Determine grey level setpoint for analysis \n",
    "        grey_lvl_sp=[x for x in grey_lvl_str if root.find(x)!=-1]\n",
    "        grey_lvl_str_add='_grey_lvl_'+grey_lvl_sp[0]+'.png'\n",
    "        #Determine ground source level\n",
    "        phase_lvl_sp=[v for k,v in phase_lvl_str.items() if root.find(k)!=-1]\n",
    "        \n",
    "        grnd_sub_repl_tple_dict={'_ground':phase_lvl_sp[0],'.png':grey_lvl_str_add}\n",
    "\n",
    "        for file in files:\n",
    "            new_file_name=repl_sub_string(file,grnd_sub_repl_tple_dict)\n",
    "            old_img=os.path.join(root,file)\n",
    "            new_img=os.path.join(root,new_file_name)\n",
    "            #renaming tuple to match srounce image\n",
    "            os.rename(old_img,new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to confirm for os path to determine method for analysis \n",
    "for root,subdir,files in os.walk(os.getcwd()):\n",
    "    if len(subdir)==2:        \n",
    "\n",
    "        dir_lst=[os.path.join(root,x) for x in subdir]\n",
    "        \n",
    "        file_lst_tple=list(zip(sorted(os.listdir(dir_lst[0])),\n",
    "                               sorted(os.listdir(dir_lst[1]))))\n",
    "                           \n",
    "        for file_src,file_grnd in file_lst_tple:\n",
    "            \n",
    "            if file_src!=file_grnd:\n",
    "                print(file_src)\n",
    "                print(file_grnd)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calls for generating file_dictionaries, file tuple sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_dir(dir_name):\n",
    "    \"\"\"Generate directory based on directory existing or not\"\"\"\n",
    "\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as error:\n",
    "            if error.errno != errno.EEXIST:\n",
    "                raise\n",
    "    with open(filepath, 'w') as my_file:\n",
    "        do_stuff(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_img_fileset_dict(src_dir,imgs_modes,key_word_sp=1,len_file_sp=0):\n",
    "    \"\"\"The purpose of this method is to create a keyword dictoinary of a folders subfolder file directory paths\"\"\"\n",
    "    file_dict={}\n",
    "    for subdir,root,files in os.walk(src_dir):\n",
    "        print(subdir)\n",
    "\n",
    "        if len(files)!=len_file_sp:\n",
    "            gt_img_name=[]\n",
    "            gt_img_name=[''.join(x) for x in imgs_modes if x in subdir.lower()]\n",
    "            #If two categories \n",
    "            if len(gt_img_name)>=key_word_sp:\n",
    "\n",
    "                tmp_file_lst=[os.path.join(subdir,file) for file in files]\n",
    "                seperator = '_'\n",
    "                seperator =seperator.join(gt_img_name)\n",
    "                file_dict[seperator]=sorted(tmp_file_lst)\n",
    "    #Sett original name for analysis and munging\n",
    "    return file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_tpl_set_lists(file_dict:dict,pair_keyword: list)->list:\n",
    "    \"\"\"The purpose of this method is to generate keyword base file tuple sets\"\"\"\n",
    "    #final list to return list\n",
    "    rtrn_lst=[]\n",
    "    #Creating list of keys to iterate through \n",
    "    file_sets=list(file_dict.keys()) \n",
    "    \n",
    "    for key in pair_keyword:\n",
    "        #ipdb.set_trace()\n",
    "        #Get main image set initially b yimage type i.e. t1dual\n",
    "        img_type_lst=[x for x in file_sets if x.lower().find(key[0])!=-1]\n",
    "        #Getting the ground key\n",
    "        img_grnd_key=[x for x in img_type_lst if x.lower().find(key[1])!=-1]\n",
    "        #Getting non ground by removing that value from the list \n",
    "        img_type_lst.remove(img_grnd_key[0])\n",
    "        #Generating file list tuple pairs for anaylysis=\n",
    "        file_pair=list(zip(file_dict[img_type_lst[0]],file_dict[img_grnd_key[0]]))\n",
    "            \n",
    "        #Appending final list of tuple for analysis\n",
    "        rtrn_lst.append(file_pair)\n",
    "            \n",
    "    return rtrn_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repl_sub_string(str_val:str,str_rep_dict:dict):\n",
    "    \"\"\"The purpose of this method is to itertivley replace substrings in a string with other values\"\"\"\n",
    "    \n",
    "    # Add escape characters to string keys for regex estimation\n",
    "    rep = dict((re.escape(k), v) for k, v in str_rep_dict.items()) \n",
    "    #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions\n",
    "    pattern = re.compile(\"|\".join(str_rep_dict.keys()))\n",
    "    \n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))],str_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_grnd_src_img(file_set_list_tuple:list,\n",
    "                                     grnd_sub_repl_tple_dict:dict,\n",
    "                                    src_sub_repl_tple_dict:dict,\n",
    "                                     dst_grnd_path:str,\n",
    "                                    dst_img_path:str):\n",
    "    \"\"\"The purpose of this method is to detail how to rename ground images to the source image and write them both \n",
    "    to a new folder. \"\"\"\n",
    "    \n",
    "    \n",
    "    for grnd_nm_path,img_nm_path in file_set_list_tuple:\n",
    "        \n",
    "        grnd_nm=os.path.basename(grnd_nm_path)\n",
    "        img_nm=os.path.basename(img_nm_path)\n",
    "        \n",
    "        strp_grnd_nm=repl_sub_string(grnd_nm,grnd_sub_repl_tple_dict)\n",
    "        strp_img_nm=repl_sub_string(img_nm,src_sub_repl_tple_dict)\n",
    "        \n",
    "        dst_grnd_path=os.path.join(dst_grnd_path,strp_grnd_nm)\n",
    "        dst_img_path=os.path.join(dst_img_path,strp_img_nm)\n",
    "        \n",
    "        copyfile(grnd_nm_path, dst_grnd_path)\n",
    "        copyfile(img_nm_path,dst_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub sampler creating folder called fold1 and fold2 for specific analysis in set folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_up_dir=os.path.join(os.path.basename(dst_directory_src),'setup')\n",
    "\n",
    "\n",
    "gen_fold_text_folder(set_up_dir,'fold1.txt',file_tuple_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fold_text_folder(dst_dir:str,txt_f_nm_trn:str,file_tuple_set:list):\n",
    "    \"\"\"Generate test file to write files to folders\"\"\"\n",
    "    final_lst=[]\n",
    "    for file in file_tuple_set:\n",
    "        if type(file) is tuple and len(file)>1:\n",
    "            \n",
    "            final_lst.append(os.path.splitext(os.path.basename(file[0]))[0])\n",
    "        else:\n",
    "            final_lst.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    dst_file=os.path.join(dst_dir,txt_f_nm_trn)\n",
    "    #Writing file of list strings to final folder. \n",
    "    \n",
    "    with open(dst_file,'w') as f_dst:\n",
    "        \n",
    "        for item in final_lst:\n",
    "            f_dst.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return final_lst\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tuple_set_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub sampler by patient to specific destination folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_directory_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub sample for U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis \n",
    "file_list_nm=os.listdir('F:\\Biomedical images\\Train\\MR_raw')\n",
    "file_list_nm_test_val=['3','8','39','2','32']\n",
    "file_list_nm_val=['3','32']\n",
    "file_list_nm_test=[x for x in file_list_nm_test_val if x not in file_list_nm_val]\n",
    "file_list_nm_train=[x for x in file_list_nm if x not in file_list_nm_test_val]\n",
    "\n",
    "samp_size=50\n",
    "src_path=r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename'\n",
    "dst_path=r'F:\\Biomedical images\\Train\\sub sample sets\\NIFTI_MR_256x256_'+str(samp_size)+'_train_Unet'\n",
    "dst_train=os.path.join(dst_path,'train')\n",
    "dst_val=os.path.join(dst_path,'valid')\n",
    "dst_test=os.path.join(dst_path,'test')\n",
    "\n",
    "pat_subsampler(src_path,dst_train,\n",
    "               file_list_nm_train,\n",
    "               samp_size)\n",
    "\n",
    "val_list_files=non_subsample_patients(dst_val,\n",
    "                                      src_path,\n",
    "                                      file_list_nm_val)\n",
    "\n",
    "test_list_files=non_subsample_patients(dst_test,\n",
    "                                       src_path,\n",
    "                                       file_list_nm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub sample for Segcaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis \n",
    "file_list_nm=os.listdir('F:\\Biomedical images\\Train\\MR_raw')\n",
    "file_list_nm_test_val=['3','8','39','2','32']\n",
    "file_list_nm_val=['3','32']\n",
    "file_list_nm_test=[x for x in file_list_nm_test_val if x not in file_list_nm_val]\n",
    "file_list_nm_train=[x for x in file_list_nm if x not in file_list_nm_test_val]\n",
    "samp_size=50\n",
    "\n",
    "#Destination to path to analysis \n",
    "dst_path=r'F:\\Biomedical images\\Train\\sub sample sets\\NIFTI_MR_256x256_'+str(samp_size)+'_train_segcaps'\n",
    "dst_train=os.path.join(dst_path,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_subsampler(r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename',\n",
    "               dst_train,\n",
    "               file_list_nm_train,samp_size)\n",
    "\n",
    "val_list_files=non_subsample_patients(dst_train,\n",
    "                                  r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename',\n",
    "                                  file_list_nm_val)\n",
    "\n",
    "test_list_files=non_subsample_patients(dst_train,\n",
    "                                  r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename',\n",
    "                                  file_list_nm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_setup_path=os.path.join(dst_path,r'train\\NIFTI_MR_256x256_png_256grey_lvl\\t1dual_inphase\\setup')\n",
    "file_setup_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_setup_path=os.path.join(dst_train,'NIFTI_MR_256x256_png_256grey_lvl\\t1dual_inphase\\setup')\n",
    "\n",
    "gen_fold_text_folder(file_setup_path,'fold2.txt',val_list_files)\n",
    "gen_fold_text_folder(file_setup_path,'test.txt',test_list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_conv_dict={0:0,63:1,126:2,189:3,252:4}\n",
    "\n",
    "src_dir=dst_dir=r'F:\\Biomedical images\\Train\\sub sample sets\\NIFTI_MR_256x256_50_train_segcaps\\train'\n",
    "\n",
    "conv_grnd_img_sing_label(src_dir,dst_dir,int_conv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub sampling patients general functions and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_directory_src,dst_directory_grnd,file_tuple_set=pat_subsampler(r'F:\\Biomedical images\\Train\\NIFTI_MR_256x256_seg_rename',\n",
    "                                                                   r'F:\\Biomedical images\\Train\\sub sample sets\\NIFTI_MR_256x256_250train',\n",
    "                                                                   file_list_nm_train,250)\n",
    "\n",
    "write_subsmpl_file(dst_directory_src,dst_directory_grnd,file_tuple_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pat_subsampler(data_src_path,data_dst_path,pat_key_word:list,\n",
    "     No_imgs:int,Pat_weights={},\n",
    "                   sampling='Uniform',pat_str='pat_id_',subdir_imgs=['masks','images']):\n",
    "    \"\"\"The purpose of this method is to subsample from each patient an even number of times to ensure no patient specific\n",
    "    bias is introduced into the process.\"\"\"\n",
    "    #Convert patient numbers to stirng \n",
    "    str_pat_no_lst=[str(x) for x in pat_key_word if type(x)==str]\n",
    "    #Confirm file arguments match requiremens\n",
    "    assert (sampling.lower() in ['uniform','weighted']), \"Correct sampling method argument uniform or weighted not inputed\"\n",
    "    #Getting different sampling method if required. \n",
    "    pat_dict=subsample_pat_weight(sampling,No_imgs,\n",
    "                                  str_pat_no_lst,\n",
    "                                  Pat_weights)\n",
    "    #ipdb.set_trace()   \n",
    "    #Iterate through work directory\n",
    "    for root,subdir,files in os.walk(data_src_path):\n",
    "        #If two subdirectories found denoted as containing ground and source proceed to second step for analysis \n",
    "        sub_dir_chk=[x for x in subdir if x.lower() in subdir_imgs]\n",
    "        \n",
    "        if len(sub_dir_chk)==2:\n",
    "            \n",
    "            dst_directory_src,dst_directory_grnd,dst_dir_setup=gen_dst_grnd_src_path(root,data_dst_path)\n",
    "            \n",
    "            file_dict=gen_img_fileset_dict(root,subdir,key_word_sp=1)\n",
    "            keys=list(file_dict.keys())\n",
    "            \n",
    "            tmp_file_set_sampl=file_dict[keys[0]]\n",
    "            \n",
    "            #Getting patient ids\n",
    "            final_pat_idxs=subsample_patient_idx(tmp_file_set_sampl,pat_dict,pat_str)\n",
    "                \n",
    "            file_grnd_files=[file_dict[keys[0]][i] for i in final_pat_idxs]\n",
    "            file_img_files=[file_dict[keys[1]][i] for i in final_pat_idxs]\n",
    "                \n",
    "            file_tuple_set=list(zip(file_img_files,\n",
    "                           file_grnd_files))\n",
    "            \n",
    "            write_subsmpl_file(dst_directory_src,dst_directory_grnd,file_tuple_set)\n",
    "            #Writing tupe set to file for analysis\n",
    "            #Generatng setup file for analysis\n",
    "            #ipdb.set_trace()\n",
    "            gen_fold_text_folder(dst_dir_setup,'fold1.txt',file_tuple_set)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_subsmpl_file(dst_directory_src,dst_directory_grnd,file_tuple_set):\n",
    "    \"\"\"Write subsampled files to file\"\"\"\n",
    "    for grnd_img,file_img in file_tuple_set:\n",
    "        \n",
    "        img_dst_b_name=os.path.basename(file_img)\n",
    "        grnd_dst_b_name=os.path.basename(grnd_img)\n",
    "        #Getting destinationnames for copying files across\n",
    "        img_dst_f_name=os.path.join(dst_directory_src,img_dst_b_name)\n",
    "        grnd_dst_f_name=os.path.join(dst_directory_grnd,grnd_dst_b_name)\n",
    "        #Writing subsampled files to new folder set. \n",
    "        copyfile(file_img,img_dst_f_name)\n",
    "        copyfile(grnd_img,grnd_dst_f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_pat_weight(sampling_arg,\n",
    "                         total_no_imgs,\n",
    "                         pat_id_lst,\n",
    "                         Pat_weights={}):\n",
    "    \"\"\"The purpose of this method is to return the number of samples that are to be\n",
    "    returned per patient class bsaed on weighted input.\"\"\"\n",
    "    if sampling_arg.lower()=='uniform':\n",
    "        img_per_pat=int(round(total_no_imgs/len(pat_id_lst),0))\n",
    "        pat_dict={k:img_per_pat for k in pat_id_lst}\n",
    "    else:\n",
    "        pat_dict={k:int(round(v*total_no_imgs,0)) for k,v in Pat_weights}\n",
    "        \n",
    "    return pat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_patient_idx(tmp_file_set_sampl:list,pat_dict:dict,pat_str='pat_id_'):\n",
    "    \"\"\"The purpose is to subsample patient ids indexes from dataset\"\"\"\n",
    "    final_pat_idxs=[]\n",
    "    #ipdb.set_trace()\n",
    "    for pat_id,sampl_no in pat_dict.items():\n",
    "        tmp_pat_str=pat_str+pat_id+'_'\n",
    "        #Getting indices of specific patient files in list\n",
    "        pat_indices = [i for i, s in enumerate(tmp_file_set_sampl) if tmp_pat_str in s.lower()]\n",
    "        #Sampling form indices for analysis\n",
    "        \n",
    "        \n",
    "        try:\n",
    "    \n",
    "            sampl_pat_ind=random.sample(pat_indices, sampl_no)\n",
    "            #Add indices to final list\n",
    "            final_pat_idxs=final_pat_idxs+sampl_pat_ind\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(str(e))\n",
    "            print('Oversampled on patient id:',tmp_pat_str)\n",
    "            print('samples available:',len(pat_indices))\n",
    "            print('samples required:',sampl_no)\n",
    "            print('Max sub sampling performed instead')\n",
    "            \n",
    "            #Add indices to final list\n",
    "            final_pat_idxs=final_pat_idxs+pat_indices\n",
    "            \n",
    "        \n",
    "    return final_pat_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dst_grnd_src_path(orig_root_dir,dst_path,file_strs={'masks':0,'images':0,'setup':0},\n",
    "                          dir_depth=-2):\n",
    "    \"\"\"The purpose of this method is to generate source and ground destination paths directories\n",
    "    for the subsampled files\"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    #Generating directory \n",
    "    sub_dir=orig_root_dir.split('\\\\')[dir_depth:]\n",
    "    #file_strs\n",
    "    for img_types,img_paths in file_strs.items():\n",
    "        \n",
    "    #Generating source directories for file analysis\n",
    "        tmp_directory=os.path.join(dst_path,*sub_dir,img_types)\n",
    "\n",
    "        if not os.path.exists(tmp_directory):\n",
    "            os.makedirs(tmp_directory)\n",
    "            \n",
    "        file_strs[img_types]=tmp_directory\n",
    "        \n",
    "    dir_grnd=file_strs['masks']\n",
    "    dir_img=file_strs['images']\n",
    "    dir_setup=file_strs['setup']\n",
    "    \n",
    "    return (dir_img,dir_grnd,dir_setup)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_per_pat=20\n",
    "path_key_word=[1,2,3,4,5,6]\n",
    "\n",
    "pat_dict={k:img_per_pat for k in path_key_word}\n",
    "pat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path=os.path.join('C://trial_path',*root.split('\\\\')[-2:],'trl')\n",
    "tr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trl_lst=[]\n",
    "\n",
    "for root,subdir,files in os.walk(os.getcwd()):\n",
    "    \n",
    "    if len(subdir)==0:\n",
    "        #print(root.split('\\\\')[-2:])\n",
    "        print(root)\n",
    "        tmp_file_set=[os.path.join(root,file) for file in files]\n",
    "    \n",
    "        trl_lst=trl_lst+tmp_file_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey level intensity methods for grey level setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytescaling(data, cmin=None, cmax=None, high=255, low=0):\n",
    "    \"\"\"\n",
    "    Converting the input image to uint8 dtype and scaling\n",
    "    the range to ``(low, high)`` (default 0-255). If the input image already has \n",
    "    dtype uint8, no scaling is done.\n",
    "    :param data: 16-bit image data array\n",
    "    :param cmin: bias scaling of small values (def: data.min())\n",
    "    :param cmax: bias scaling of large values (def: data.max())\n",
    "    :param high: scale max value to high. (def: 255)\n",
    "    :param low: scale min value to low. (def: 0)\n",
    "    :return: 8-bit image data array\n",
    "    \"\"\"\n",
    "    if data.dtype == np.uint8:\n",
    "        return data\n",
    "\n",
    "    if high > 255:\n",
    "        high = 255\n",
    "    if low < 0:\n",
    "        low = 0\n",
    "    if high < low:\n",
    "        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n",
    "\n",
    "    if cmin is None:\n",
    "        cmin = data.min()\n",
    "    if cmax is None:\n",
    "        cmax = data.max()\n",
    "\n",
    "    cscale = cmax - cmin\n",
    "    if cscale == 0:\n",
    "        cscale = 1\n",
    "\n",
    "    scale = float(high - low) / cscale\n",
    "    bytedata = (data - cmin) * scale + low\n",
    "    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_igs(im, levels, qtype='uniform', maxCount=255, displayLevels=None):\n",
    "    \"\"\"\n",
    "    Function to run uniform gray-level and improved gray-scale Quantization.\n",
    "    This takes in an image, and buckets the gray values depending on the params.\n",
    "    Args:\n",
    "        im (array): image to be quantized as an array of values from 0 to 255\n",
    "        levels (int): number of levels to quantize to.\n",
    "            This should be a positive integer, and smaller than the maxCount.\n",
    "        qtype (optional[string]): the type of quantization to perform.\n",
    "            Can be either 'uniform' or 'igs'; Defaults to 'uniform'.\n",
    "        maxCount (optional[int]): the maximum value for a digital count\n",
    "        displayLevels (optional[int]): the number of gray levels to expand to.\n",
    "            By default this value is None and will shrink the range of greys.\n",
    "            This value should be a positive integer when provided.\n",
    "    Return:\n",
    "        the quantized image\n",
    "    \"\"\"\n",
    "    # default value if we need to return early\n",
    "    returnImage = im\n",
    "\n",
    "    # get int type\n",
    "    dtype = im.dtype\n",
    "\n",
    "    if (displayLevels == None):\n",
    "        # by default don't re-expand the image\n",
    "        displayCount = levels\n",
    "    elif displayLevels > 0:\n",
    "        displayCount = displayLevels-1\n",
    "    else:\n",
    "        print(\"displayLevels is an invalid value\")\n",
    "        return returnImage\n",
    "\n",
    "    # we're getting one more level than we should be, so minus 1\n",
    "    if ((levels > 0) and (levels < maxCount)):\n",
    "        levels = levels - 1\n",
    "    else:\n",
    "        print(\"levels needs to be a positive value, and smaller than the maxCount\")\n",
    "        return returnImage\n",
    "\n",
    "    if (qtype == 'uniform'):\n",
    "        # uniform method from lecture\n",
    "        returnImage = np.floor((im/((maxCount+1)/float(levels))))*(displayCount/levels)\n",
    "\n",
    "    elif (qtype == 'igs'):\n",
    "        # error diffusion method from lecture\n",
    "\n",
    "        # default error as 0 for the first pixel\n",
    "        error = 0\n",
    "\n",
    "        # the list of rows that will be turned into an image\n",
    "        returnList = []\n",
    "        for i in range(len(im)):\n",
    "            returnRow = []\n",
    "            for j in range(len(im[i])):\n",
    "                # get a new digital count with the error\n",
    "                errDC = im[i][j] + error\n",
    "                # save the error for the next pixel\n",
    "                error = errDC % (maxCount/levels)\n",
    "\n",
    "                # calculate the new digital count, and append it to the row\n",
    "                newDC = np.floor((errDC)/(maxCount/levels))\n",
    "                returnRow.append(newDC*(displayCount/levels))\n",
    "            # append the row to the final image\n",
    "            returnList.append(np.array(returnRow))\n",
    "\n",
    "        returnImage = np.array(returnList, dtype)\n",
    "\n",
    "    else:\n",
    "        # invalid qtype\n",
    "        print('qtype is an invalid value, please use \"uniform\", or \"igs\"')\n",
    "\n",
    "   \n",
    "    return np.array(returnImage, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine sub sampling based on class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_class_dict={}\n",
    "for file in files_list:\n",
    "    #\n",
    "    tmp_key_set={0:0,126:0,252:0,63:0,184:0}\n",
    "    #Assess file name and type\n",
    "    tmp_file=imageio.imread(file)\n",
    "    uniq_val_class_bal=np.unique(tmp_file)\n",
    "    tuple_set_vals=list(zip(list(uniq_val_class_bal[0]),\n",
    "                      list(uniq_val_class_bal[1])))\n",
    "    for int_val,cnt in tuple_set_vals:\n",
    "        if int_val in tmp_key_set.keys():\n",
    "            tmp_key_set[int_val]=cnt\n",
    "    #Ordering dictionary based on values\n",
    "    ord_dict = OrderedDict(sorted(trl_dict.items()))\n",
    "    arr_val=np.array(list(od.values()))\n",
    "    arr_val_norm=arr_val/arr_val.sum()\n",
    "    #Creating unique key value for class balance to file name. \n",
    "    file_class_dict[arr_val_norm]=file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 1 in trl_dict.keys():\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trl_dict={1:2,3:4,66:4}\n",
    "od = OrderedDict(sorted(trl_dict.items()))\n",
    "arr_val=np.array(list(od.values()))\n",
    "arr_val_norm=arr_val/arr_val.sum()\n",
    "arr_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
